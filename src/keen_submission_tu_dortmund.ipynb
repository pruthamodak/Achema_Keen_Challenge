{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1a59d86",
   "metadata": {},
   "source": [
    "# Keen HACKATHON : AI for automatic process supervision, TU Dortmund\n",
    "\n",
    "### Team Members :\n",
    "* Sourabh (sourabhranade96@gmail.com)\n",
    "* Prutha Modak (pruthamodak@gmail.com)\n",
    "* Kavyashree Renukachari (kavyashree94v@gmail.com)\n",
    "* Sayali Barve (sayalibarve.040@gmail.com)\n",
    "* Kartik Kadur (karthikkr36@gmail.com)\n",
    "\n",
    "The work done in this notebook is also available as a project in the gitlab repository :\n",
    "\n",
    "[KEEN Hackathon](https://gitlab.com/keenchallenge1/keenchallenge.git)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f877610",
   "metadata": {},
   "source": [
    "#### Packages used in this notebook can be installed from the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbe56cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install necessary packages if not installed\n",
    "!pip install tqdm\n",
    "!pip install numpy\n",
    "!pip install Pillow\n",
    "!pip install torch==1.7.1+cpu torchvision==0.8.2+cpu torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f430a455",
   "metadata": {},
   "source": [
    "#### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39cbed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "from torch.nn import init\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from random import shuffle\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5441ec9",
   "metadata": {},
   "source": [
    "### Set the proper device for execution\n",
    "Set the proper device for moving the tensors and model while training/evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afd8807",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a766772a",
   "metadata": {},
   "source": [
    "### Model\n",
    "The below cell defines model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ef215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import os\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                in_channels: int, \n",
    "                out_channels: int,\n",
    "                kernel_size: int=3, \n",
    "                stride: int=1,\n",
    "                padding=0,\n",
    "                bias: bool = True) -> None:\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        x = self.conv(inp)\n",
    "        x = self.bn(x)\n",
    "        return self.relu(x)\n",
    "\n",
    "class KeenModel(nn.Module):\n",
    "    def __init__(self, num_classes, input_shape, factor=4):\n",
    "        super(KeenModel, self).__init__()\n",
    "\n",
    "        input_channels = 3\n",
    "        # initial convolution\n",
    "        self.conv1 = ConvBlock(input_channels, 64//factor, stride=4, padding=1)\n",
    "        self.conv2 = ConvBlock(64//factor, 128 // factor, stride=2, padding=1)\n",
    "        self.conv3 = ConvBlock(128 // factor, 256 // factor, stride=2, padding=1)\n",
    "        self.conv4 = ConvBlock(256 // factor, 512 // factor, stride=2, padding=1)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear((512//factor) * (input_shape//64) * (input_shape//64), 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                if m.bias is not None:\n",
    "                    init.uniform_(m.bias)\n",
    "                init.kaiming_uniform_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def keen_model(pretrained=True, num_classes=2, input_shape=256, factor=4):\n",
    "    import urllib\n",
    "    try:\n",
    "        model = KeenModel(num_classes, input_shape, factor)\n",
    "        if pretrained:\n",
    "            ckpt_path, _ = urllib.request.urlretrieve(\"https://drive.google.com/file/d/1-0Ai-Tqbd4CC0EZPKvcWbuaDqTgWe_ML/view?usp=sharing\", os.path.join(os.getcwd(), \"checkpoint.pth\"))\n",
    "            print(f\"Trying to download checkpoint under : {ckpt_path}\")\n",
    "            checkpoint = torch.load(os.path.join(os.getcwd(), \"checkpoint.pth\"), map_location='cpu')\n",
    "            if 'state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "    except BaseException as e:\n",
    "        print(\"Error downloading checkpoint. Please download it manually and load from the following url:\")\n",
    "        print(\"https://drive.google.com/file/d/1-0Ai-Tqbd4CC0EZPKvcWbuaDqTgWe_ML/view?usp=sharing\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e51b2e",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "Below cell defines a dataloader for efficient loading of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe0ee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeenDataloader():\n",
    "    def __init__(self, root, is_training=False, transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.is_training = is_training\n",
    "        self.images = self.get_paths(root)\n",
    "        self.labels = {\"Fluten\" : 0, \"Normalzustand\" : 1}\n",
    "    \n",
    "    def get_paths(self, root):\n",
    "        img_format = ['.jpg', '.png']\n",
    "\n",
    "        dirs = [x[0] for x in os.walk(root, followlinks=True) if not x[0].startswith('.')]\n",
    "        datasets = []\n",
    "        for fdir in dirs:\n",
    "            for el in os.listdir(fdir):\n",
    "                if os.path.isfile(os.path.join(fdir, el)) and \\\n",
    "                not el.startswith('.') and \\\n",
    "                any([el.endswith(ext) for ext in img_format]):\n",
    "                    datasets.append(os.path.join(fdir,el))\n",
    "        shuffle(datasets)\n",
    "        return datasets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        image = Image.open(image)\n",
    "        if self.is_training:\n",
    "            if self.transforms is None:\n",
    "                self.transforms = transforms.Compose([\n",
    "                                                    transforms.Resize((256, 256)),\n",
    "                                                    transforms.RandomAffine([20,50]),\n",
    "                                                    transforms.RandomRotation([30,70]),\n",
    "                                                    transforms.RandomVerticalFlip(0.5),\n",
    "                                                    transforms.RandomHorizontalFlip(0.5),\n",
    "                                                    transforms.ToTensor(),\n",
    "                                                    transforms.Normalize((0.5021, 0.4781, 0.4724), (0.3514, 0.3439, 0.3409)),\n",
    "                                                      ])    \n",
    "        else:\n",
    "            self.transforms = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor(),\n",
    "                                                transforms.Normalize((0.5021, 0.4781, 0.4724), (0.3514, 0.3439, 0.3409)),])\n",
    "        image = self.transforms(image)\n",
    "        return {'image' : image, 'label' : torch.tensor(self.labels[os.path.basename(os.path.dirname(self.images[index]))], dtype=torch.int64)}  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eec7d16",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0168cc",
   "metadata": {},
   "source": [
    "### Test model using pretrained weights.\n",
    "The pretrained weights for the model is available and unloaded under the google drive link :\n",
    "[checkpoint](https://drive.google.com/file/d/1-0Ai-Tqbd4CC0EZPKvcWbuaDqTgWe_ML/view?usp=sharing)\n",
    "\n",
    "\n",
    "To test the models performance using the pretrained weights, please use the function ```test_step(img_dir, ckpt_path)```,\n",
    "defined in the cell below. The function ```test_step(img_dir, ckpt_path)``` accepts the path to the root directory of the test dataset.\n",
    "\n",
    "If the checkpoint is available locally, pass its path as an argument to the ```ckpt_path``` parameter in ```test_step(img_dir, ckpt_path)``` function.\n",
    "\n",
    "If this argument is ```None```, the function tries to download the checkpoint from the google drive link provided above.\n",
    "\n",
    "In our validation process, our model acheved an Validation accuracy of 95 %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff59fa80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_config = {'batch_size': 16,\n",
    "               'num_workers': 0,\n",
    "               'pin_memory': True, \n",
    "               'drop_last': True}\n",
    "\n",
    "def test_step(img_dir, ckpt_path=None):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    if ckpt_path is not None:\n",
    "        model = KeenModel(2, 256)\n",
    "        # load checkpoint\n",
    "        checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "        if 'state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "    else:\n",
    "        model = keen_model(True)\n",
    "    model = model.to(device)\n",
    "    # create criterion\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    # create testset\n",
    "    testset = KeenDataloader(img_dir, is_training=False)\n",
    "    testloader = DataLoader(testset, **test_config)\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    iteration = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in tqdm(testloader):\n",
    "        iteration+=1\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data['image'].to(device), data['label'].to(device)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        total += labels.size(0)\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        test_loss += loss.item()\n",
    "    print('[%5d] Test loss: %.3f, Test Accuracy: %.3f' %\n",
    "                    (iteration, test_loss/iteration, correct/total))\n",
    "    logging.info(f'Test completed')\n",
    "    return test_loss/iteration, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa222c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = test_step(\"C:\\\\Users\\\\Karthik\\\\Documents\\\\KEEN_DATA\\\\Validation\")\n",
    "print(loss, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ed0c1a",
   "metadata": {},
   "source": [
    "### Predict on a single image\n",
    "There is a possibility to predict on a single image sample using the ```predict(image_path, ckpt_path)``` function. Just pass the path of the image as an argument and the function returns the predicted label.\n",
    "\n",
    "The loader and index are mapped as follows:\n",
    "* 0 : Fluten\n",
    "* 1 : Normalzustand\n",
    "\n",
    "Again pass the locally downloaded checkpoint path to the ```ckpt_path``` argument to use locally available checkpoint.\n",
    "\n",
    "If this argument is ```None```, the function tries to download the checkpoint from the google drive link provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dd777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image_path, ckpt_path=None):\n",
    "    label = {0 : \"Fluten\", 1: \"Normalzustand\"}\n",
    "    # create model\n",
    "    if ckpt_path is not None:\n",
    "        model = KeenModel(2, 256)\n",
    "        # load checkpoint\n",
    "        checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "        if 'state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "    else:\n",
    "        model = keen_model(True)\n",
    "    # basic transformations\n",
    "    transform = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor(),])\n",
    "    # open image\n",
    "    image = Image.open(image_path)\n",
    "    image = transform(image)\n",
    "    image = torch.unsqueeze(image, dim=0)\n",
    "    model = model.eval() \n",
    "    outputs = model(image)\n",
    "    _, prediction = torch.max(outputs, dim=1)\n",
    "    return label[int(prediction)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d747e3",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a793e6",
   "metadata": {},
   "source": [
    "### Run Configurations\n",
    "The following block contains the run configurations that can be set while training the model. \n",
    "\n",
    "We have trained the model with the configurations shown below:\n",
    "\n",
    "* 'train_path' : \"path/to/dataroot/Training\",\n",
    "*  'val_path' : \"path/to/dataroot/Validation\",\n",
    "*    'epochs' : 50,\n",
    "*    'lr' : 0.0001,\n",
    "*    'wd' : 0,\n",
    "*    'batch_size' : 16,\n",
    "*    'val_batch_size' : 16,\n",
    "*    'num_workers' : 0,\n",
    "*    'save_root' : \"path/to/savedirectory\",\n",
    "*    'checkpoint' : \"path/to/savedirectory/checkpoint\",\n",
    "*    'logs_root' : \"path/to/savedirectory/logs\",\n",
    "*    'resume' : \"path/to/checkpoint\",\n",
    "*    'print_freq' : 200,\n",
    "*    'save_freq' : 5,\n",
    "*    'val_freq' : 5,\n",
    "*    'initial_eval' : False,\n",
    "*    'is_training' : True\n",
    "\n",
    "Please cheange the values according to your requirements. If checkpoint path is None, the the model is initialized with random initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e941f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'train_path' : \"C:\\\\Users\\\\Karthik\\\\Documents\\\\KEEN_DATA\\\\Training\",\n",
    "    'val_path' : \"C:\\\\Users\\\\Karthik\\\\Documents\\\\KEEN_DATA\\\\Validation\",\n",
    "    'epochs' : 20,\n",
    "    'lr' : 0.0001,\n",
    "    'wd' : 0.0001,\n",
    "    'batch_size' : 64,\n",
    "    'val_batch_size' : 64,\n",
    "    'num_workers' : 0,\n",
    "    'save_root' : \"C:\\\\Users\\\\Karthik\\\\Desktop\\\\experiments\\\\AVGPOOL\",\n",
    "    'checkpoint' : \"C:\\\\Users\\\\Karthik\\\\Desktop\\\\experiments\\\\AVGPOOL\\\\checkpoint\",\n",
    "    'logs_root' : \"C:\\\\Users\\\\Karthik\\\\Desktop\\\\experiments\\\\AVGPOOL\\\\logs\",\n",
    "    'resume' : None,\n",
    "    'print_freq' : 100,\n",
    "    'save_freq' : 1,\n",
    "    'val_freq' : 2,\n",
    "    'initial_eval' : False,\n",
    "    'is_training' : True,\n",
    "    'lr_factor' : 0.1,\n",
    "    'lr_step_size' : 7,\n",
    "    'start_epoch' : 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c1e8dc",
   "metadata": {},
   "source": [
    "### Functions required for training\n",
    "The below cells contain the functions required for training the model\n",
    "\n",
    "### Functions to create dataloaders and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5accd737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(config):\n",
    "    train_set = KeenDataloader(config['train_path'], is_training=True)\n",
    "    val_set = KeenDataloader(config['val_path'], is_training=False)\n",
    "    tkwargs = {'batch_size': config['batch_size'],\n",
    "               'num_workers': config['num_workers'],\n",
    "               'pin_memory': True, 'drop_last': True}\n",
    "    trainloader = DataLoader(train_set, **tkwargs)\n",
    "    tkwargs = {'batch_size': config['val_batch_size'],\n",
    "               'num_workers': config['num_workers'],\n",
    "               'pin_memory': True, 'drop_last': True}\n",
    "    valloader = DataLoader(val_set, **tkwargs)\n",
    "    return trainloader, valloader\n",
    "\n",
    "def create_model_and_optimizer():\n",
    "    model = KeenModel(2, 256).to(device)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config['wd'])\n",
    "\n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f8cf84",
   "metadata": {},
   "source": [
    "### Functions to load pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d2d9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(model, optimizer, config):\n",
    "    if config[\"resume\"] is not None : \n",
    "        checkpoint = torch.load(config['resume'], map_location='cpu')\n",
    "        if 'state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "        if 'optimizer' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959e11ab",
   "metadata": {},
   "source": [
    "### Functions to perform train step and val step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d702a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, criterion, optimizer, trainloader, epoch):\n",
    "    running_loss = 0.0\n",
    "    iteration = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in tqdm(trainloader):\n",
    "        iteration+=1\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data['image'].to(device), data['label'].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # calculate acc\n",
    "        total += labels.size(0)\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if iteration % config['print_freq'] == 0:\n",
    "            logging.info('[%d, %5d] loss: %.3f, accuracy: %.3f' %\n",
    "                (epoch + 1, iteration, running_loss/iteration, correct/total))\n",
    "    logging.info('[%d, %5d] Epoch loss: %.3f, Accuracy: %.3f' %\n",
    "                    (epoch + 1, iteration, running_loss/iteration, correct/total))\n",
    "    logging.info(f'Epoch {epoch} completed')\n",
    "    return running_loss/iteration, correct/total\n",
    "\n",
    "def val_step(model, criterion, valloader):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    iteration = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in tqdm(valloader):\n",
    "        iteration+=1\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data['image'].to(device), data['label'].to(device)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        total += labels.size(0)\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        val_loss += loss.item()\n",
    "    logging.info('[%5d] Validation loss: %.3f, Validation Accuracy: %.3f' %\n",
    "                    (iteration, val_loss/iteration, correct/total))\n",
    "    logging.info(f'Validation completed')\n",
    "    return val_loss/iteration, correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715ce27e",
   "metadata": {},
   "source": [
    "### Function to run train epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda2090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, model, criterion, optimizer, trainloader, valloader):\n",
    "    metrics = {'train_loss' : [], 'train_acc' : [], 'val_loss' : [], 'val_acc' : []}\n",
    "    model.train()\n",
    "    # le scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, config['lr_step_size'])\n",
    "    for epoch in range(config['start_epoch'], epochs):\n",
    "        logging.info(f\"Current learning rate : {scheduler.get_last_lr()}\")\n",
    "        # train a single epoch\n",
    "        train_loss, train_acc = train_step(model, criterion, optimizer, trainloader, epoch)\n",
    "        # validation\n",
    "        if epoch % config['val_freq'] == 0:\n",
    "            val_loss, val_accuracy = val_step(model, criterion, valloader)\n",
    "            metrics['val_loss'].append(val_loss)\n",
    "            metrics['val_acc'].append(val_accuracy)\n",
    "        # lr schedule\n",
    "        scheduler.step()\n",
    "        # update metrics\n",
    "        metrics['train_loss'].append(train_loss)\n",
    "        metrics['train_acc'].append(train_acc)\n",
    "        # save model\n",
    "        if epoch % config['save_freq'] == 0:\n",
    "            save_model(model, optimizer, epoch, config)\n",
    "            logging.info(f\"Model saved under : {os.path.join(config['save_root'], f'ckpt_epoch_{epoch}.pth')}\")\n",
    "    # save at the end of epoch\n",
    "    save_model(model, optimizer, epoch, config)\n",
    "    logging.info(f\"Model saved under : {os.path.join(config['save_root'], f'ckpt_epoch_{epoch}.pth')}\")\n",
    "    return metrics\n",
    "\n",
    "def save_model(model, optimizer, epoch, config):\n",
    "    # save checkpoint\n",
    "    model_optim_state = {'epoch': epoch,\n",
    "                         'state_dict': model.state_dict(),\n",
    "                         'optimizer': optimizer.state_dict(),\n",
    "                         }\n",
    "    model_name = os.path.join(\n",
    "        config['checkpoint'], 'ckpt_epoch_%03d_.pth' % (\n",
    "            epoch))\n",
    "    torch.save(model_optim_state, model_name)\n",
    "    logging.info('saved model {}'.format(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01813d6",
   "metadata": {},
   "source": [
    "## Steps to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd59a275",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(config['save_root'], exist_ok=True)\n",
    "os.makedirs(config['logs_root'], exist_ok=True)\n",
    "os.makedirs(config['checkpoint'], exist_ok=True)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                filename=os.path.join(config['logs_root'], 'stdout.log'),\n",
    "                format='%(asctime)s %(message)s')\n",
    "logging.info('Run configurations:')\n",
    "for item in config:\n",
    "    logging.info(f\"{item} : {config[item]}\")\n",
    "logging.info(\"Creating dataloaders\")\n",
    "trainloader, valloader = create_dataloader(config)\n",
    "logging.info(\"Creating model, optimizer and criterion functions\")\n",
    "model, criterion, optimizer = create_model_and_optimizer()\n",
    "\n",
    "if config[\"initial_eval\"]:\n",
    "    val_loss = val_step(model, criterion, valloader)\n",
    "logging.info(f\"Training model on {device}:\")\n",
    "metrics = train(config['epochs'], model, criterion, optimizer, trainloader, valloader)\n",
    "with open(os.path.join(config['save_root'], 'metrics.pkl'), 'wb') as pkl:\n",
    "    pickle.dump(metrics, pkl, pickle.HIGHEST_PROTOCOL)\n",
    "logging.info('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
